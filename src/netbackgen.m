classdef netbackgen < mlp
% General neural net w/ differentiable transfer trained using backpropagation.
    properties
        nepoch    = 10;       % Number of epochs.
        batchsize = 1000;     % Number of examples in each batch.
        nupdate  = 10;       % Number of linesearches/updates per batch.
    end
    properties (Dependent = true)
        nu        % Learning rate.
        mu        % Weight decay.
        momentum  % Momentum parameter.
        ErrorFcns % Error function of each layer (used for training).
    end
    properties (Access = 'private')
        learning_rate  % nu
        decay_rate     % mu
        momentum_param % momentum
        
    end
    % ==================================================
    % PUBLIC METHODS
    % ==================================================
    methods
        function self = netbackgen(varargin)
            % Usage: netbp(arch, data) OR netbp(fullarch, TransferFcns, ErrorFcns)
            [fullarch, TransferFcns, ErrorFcns] = netbackgen.processArgs(varargin{:});
            % Instantiate
            self = self@mlp(fullarch, TransferFcns);
            self.ErrorFcn = ErrorFcns{end}; % Should usually be the same.
            self.ErrorFcns = ErrorFcns;     % Error functions for each layer.
            % Initialize weights
            self = initialize(self);        % Initialize weights.
        end
        function self = train(self, data)
            % Get batchdata
            if ischar(data), self.dataset=data; data = loaddata(data); end;
            if size(data.target, 2) ~= self.NumOutputs
                error('Data and architecture size do not agree.')
            end
            batchdata = makeBatchData(data, self.batchsize);
            nbatch = length(batchdata);
            % Record initial error.
            if isempty(self.ErrorTrain)
                self = recordErrors(self, data);
                printstatus(self);
            end
            % Iterate
            for i = 1:self.nepoch
                for j = 1:nbatch
                    % Forward pass
                    X = getStochasticActivations(self, batchdata{j}.input);
                    % Backward pass
                    Y = getStochasticBackgen(self, batchdata{j}.target);
                    % Update
                    for k = 1:self.nlayers
                       % Layer input is forward activation.
                       if k == 1
                           layerdata.input = batchdata{j}.input;
                       else
                           layerdata.input = X{k-1};
                       end
                       % Layer target is backward activation.
                       if k == self.nlayers
                           layerdata.target = batchdata{j}.target;
                       else
                           layerdata.target = Y{self.nlayers - k};
                       end
                       % Gradient: increases loglikelihood of sigmoid net.
                       % Do not include bias term.
                       self.layers{k} = train_gd_nobias(self.layers{k}, layerdata, self.nupdate);
                    end
                end % batch
                % Record errors
                self = recordErrors(self, data);
                printstatus(self);
            end % epoch
        end
        function Y = getStochasticBackgen(self, data)
            % Returns stochastic activations at each layer, generated by
            % back propagating signal from output of network.
            if ischar(data), data = loaddata(data); data = data.target; end;
            Y = cell(1, self.nlayers);
            Y{1} = apply_sigmoid_stochastic_reverse(self.layers{end}, data);
            for i = 2:self.nlayers
                Y{i} = apply_sigmoid_stochastic_reverse(self.layers{self.nlayers-i+1}, Y{i-1});
            end
        end      
        % Properties
        function ErrorFcns = get.ErrorFcns(self)
            % Get ErrorFcn property of each layer perceptron.
            ErrorFcns = cell(self.nlayers,1);
            for i = 1:self.nlayers
                ErrorFcns{i} = self.layers{i}.ErrorFcn;
            end
        end
        function self = set.ErrorFcns(self, ErrorFcns)
            % Set ErrorFcn property of each layer perceptron.
            if ~iscell(ErrorFcns)
                % Assign this error function to all layers.
                assert(ischar(ErrorFcns));
                ErrorFcns = repmat({ErrorFcns}, 1, self.nlayers);
            end
            for i = 1:self.nlayers
               self.layers{i}.ErrorFcn = ErrorFcns{i};
            end
            % Check
            if ~strcmp(self.layers{end}.ErrorFcn, self.ErrorFcn)
                warning('Training ErrorFcns{end} ~= ErrorFcn. Make sure you know what you are doing.');
            end
        end
        function nu = get.nu(self)
            % Returns high level nu.
            nu = self.learning_rate;
        end
        function self = set.nu(self, nu)
            % Set adjusted nu for all layers; keep track of nu in this
            % object too.
            self.learning_rate = nu;
            % Set parameters layer. Scale nu by number of inputs.
            for k = 1:self.nlayers
                %self.layers{k}.nu = self.layers{k}.calcAdjustednu(self.nu);
                self.layers{k}.nu = nu;
            end
        end
        function mu = get.mu(self)
            % Returns high level mu.
            mu = self.decay_rate;
        end
        function self = set.mu(self, mu)
            % Set mu for all layers; keep track of mu in this
            % object too.
            self.decay_rate = mu;
            % Set parameters layer. Scale nu by number of inputs.
            for k = 1:self.nlayers
                %self.layers{k}.nu = self.layers{k}.calcAdjustednu(self.nu);
                self.layers{k}.mu = mu;
            end
        end
        function momentum = get.momentum(self)
            % Return high-level momentum.
            momentum = self.momentum_param;
        end
        function self = set.momentum(self, momentum)
            % Set momentum term in each layer.
            self.momentum_param = momentum;
            for k = 1:self.nlayers
                %self.layers{k}.nu = self.layers{k}.calcAdjustednu(self.nu);
                self.layers{k}.momentum = momentum;
            end
        end
        % Output
        function string = param2str(self)
            % Creates short string describing parameters of netdt.
            string = sprintf('backgen_bs%dnup%d', self.batchsize, self.nupdate);
        end

    end
    % ==================================================
    % PRIVATE METHODS
    % ==================================================
    methods (Access='private')
    end
    % ==================================================
    % STATIC METHODS
    % ==================================================
    methods (Static = true, Access='protected')
        function [fullarch, TransferFcns, ErrorFcns] = processArgs(varargin)
            % Process various forms of input for constructor.
            % INPUT: (arch, data)
            %     OR (fullarch, TransferFcns, ErrorFcns)
            % OUTPUT:
            %   fullarch = vector specifying n0,...,nl
            %   TransferFcns = cellarray specifying all transfer functions
            %   ErrorFcns = Functions for training each layer.
            if nargin == 2
                % Convienient constructor from hidden arch and data.
                arch = varargin{1};
                data = varargin{2};
                % fullarch
                if ischar(data), data = loaddata(data); end;
                fullarch = [size(data.input,2), arch, size(data.target,2)];
                % TransferFcns
                [TransferFcns, ErrorFcn] = mlp.detectFcns(fullarch, data);
                % ErrorFcns
                ErrorFcns = repmat({'CrossEntropyError'}, 1, length(fullarch)-1);
                ErrorFcns{end} = ErrorFcn;
            elseif nargin == 3
                % Fully specified.
                fullarch = varargin{1};
                TransferFcns = varargin{2};
                ErrorFcns = varargin{3};
                % If single TransferFcn is given, assume it for all.
                if ~iscell(TransferFcns) || (iscell(TransferFcns) && length(TransferFcns)==1)
                    % Assign this transfer function to every layer.
                    assert(ischar(TransferFcns));
                    TransferFcns = repmat({TransferFcns}, 1, length(fullarch)-1);
                end
                % If single ErrorFcn is given, assume it for all.
                if ~iscell(ErrorFcns) || (iscell(ErrorFcns) && length(ErrorFcns)==1)
                    % Assign this ErrorFcn function to every layer.
                    assert(ischar(ErrorFcns));
                    ErrorFcns = repmat({ErrorFcns}, 1, length(fullarch)-1);
                end
            else
                error('Input error.')
            end
        end
    end
end
% ==================================================
% OTHER METHODS
% ==================================================

    